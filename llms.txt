# AiCore LLM Interface Documentation

> A unified Python framework for interacting with multiple LLM providers, embeddings providers, and MCP servers with built-in observability and cost tracking.

---

## Installation

```bash
pip install core-for-ai
```

### Optional Extras

```bash
# Install with all optional dependencies
pip install core-for-ai[all]

# Install with dashboard support (Plotly, Dash)
pip install core-for-ai[dashboard]

# Install with SQL support (SQLAlchemy, async drivers)
pip install core-for-ai[sql]

# Install the Claude Code proxy server (FastAPI + SSE)
pip install core-for-ai[claude-server]
```

### Requirements
- Python 3.11, 3.12, or 3.13
- Pydantic v2

---

## Basic LLM Module

### Imports

```python
from aicore.llm import Llm
from aicore.llm.config import LlmConfig
```

### Synchronous Completion

```python
from aicore.llm import Llm
from aicore.llm.config import LlmConfig

# Initialize with configuration
config = LlmConfig(
    provider="openai",
    api_key="your_api_key",
    model="gpt-4o"
)

llm = Llm.from_config(config)

# Generate completion (streams by default)
response = llm.complete("Hello, how are you?")
print(response)

# With system prompt
response = llm.complete(
    "Write a poem about technology",
    system_prompt="You are a creative poet"
)

# With JSON output
response = llm.complete(
    "List top 3 programming languages",
    json_output=True  # Returns dict
)

# Without streaming
response = llm.complete("Explain AI", stream=False)
```

### Asynchronous Completion

```python
import asyncio
from aicore.llm import Llm
from aicore.llm.config import LlmConfig

async def main():
    config = LlmConfig(
        provider="openai",
        api_key="your_api_key",
        model="gpt-4o"
    )
    
    llm = Llm.from_config(config)
    
    # Async completion
    response = await llm.acomplete("Hello, how are you?")
    print(response)
    
    # With system prompt
    response = await llm.acomplete(
        "Analyze this data",
        system_prompt="You are a data analyst"
    )
    
    # With streaming (default)
    response = await llm.acomplete("Explain quantum computing", stream=True)

asyncio.run(main())
```

### Complete Method Signatures

```python
# Synchronous
def complete(
    prompt: Union[str, BaseModel, RootModel],
    system_prompt: Optional[Union[str, List[str]]] = None,
    prefix_prompt: Optional[Union[str, List[str]]] = None,
    img_path: Optional[Union[str, Path, bytes, List[Union[str, Path, bytes]]]] = None,
    json_output: bool = False,
    stream: bool = True,
    agent_id: Optional[str] = None,
    action_id: Optional[str] = None
) -> Union[str, Dict]

# Asynchronous
async def acomplete(
    prompt: Union[str, List[str], List[Dict[str, str]], BaseModel, RootModel],
    system_prompt: Optional[Union[str, List[str]]] = None,
    prefix_prompt: Optional[Union[str, List[str]]] = None,
    img_path: Optional[Union[str, Path, bytes, List[Union[str, Path, bytes]]]] = None,
    json_output: bool = False,
    stream: bool = True,
    as_message_records: bool = False,
    agent_id: Optional[str] = None,
    action_id: Optional[str] = None
) -> Union[str, Dict, List[Union[str, Dict[str, str]]]]
```

---

## LLM Configuration

### LlmConfig Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `provider` | Literal["anthropic", "gemini", "groq", "mistral", "nvidia", "openai", "openrouter", "deepseek", "grok", "zai", "claude_code", "remote_claude_code"] | Yes | LLM provider identifier |
| `model` | str | Yes | Model name (e.g., "gpt-4o", "claude-3-5-sonnet-20241022") |
| `api_key` | Optional[str] | No | API key (can use env vars instead) |
| `base_url` | Optional[str] | No | Custom API endpoint |
| `temperature` | float | No | Sampling temperature (0-1, default: 0) |
| `max_tokens` | int | No | Max completion tokens (default: 12000) |
| `timeout` | Optional[int] | No | Request timeout in seconds |
| `mcp_config_path` | Optional[str] | No | Path to MCP configuration JSON |
| `tool_choice` | Union[str, Dict, None] | No | Tool calling behavior |
| `max_tool_calls_per_response` | Optional[int] | No | Limit tool calls per response |
| `concurrent_tool_calls` | Optional[bool] | No | Allow parallel tool calls (default: True) |
| `reasoner` | Optional[LlmConfig] | No | Reasoning model configuration |
| `pricing` | Optional[PricingConfig] | No | Custom pricing configuration |
| `permission_mode` | Optional[str] | No | Claude Code permission mode (`"default"`, `"acceptEdits"`, `"bypassPermissions"`) — `claude_code` / `remote_claude_code` only |
| `cwd` | Optional[str] | No | Working directory for Claude Code session — `claude_code` / `remote_claude_code` only |
| `max_turns` | Optional[int] | No | Max agentic turns per call — `claude_code` / `remote_claude_code` only |
| `allowed_tools` | Optional[List[str]] | No | Allowlisted tool names for Claude Code — `claude_code` / `remote_claude_code` only |
| `cli_path` | Optional[str] | No | Override path to `claude` binary — `claude_code` only |
| `skip_health_check` | Optional[bool] | No | Skip proxy `/health` check on init — `remote_claude_code` only |

### YAML Configuration

```yaml
# config/config.yml
llm:
  provider: "openai"
  api_key: "your_openai_api_key"
  model: "gpt-4o"
  temperature: 0.7
  max_tokens: 2048
  mcp_config_path: "./mcp_config.json"
  max_tool_calls_per_response: 3

  # Optional reasoning augmentation
  reasoner:
    provider: "groq"
    api_key: "your_groq_api_key"
    model: "deepseek-r1-distill-llama-70b"
    temperature: 0.5
    max_tokens: 1024
```

### Programmatic Configuration

```python
from aicore.llm.config import LlmConfig

# Basic configuration
config = LlmConfig(
    provider="anthropic",
    api_key="your_anthropic_key",
    model="claude-3-5-sonnet-20241022",
    temperature=0.7,
    max_tokens=4096
)

# With MCP support
config = LlmConfig(
    provider="openai",
    api_key="your_openai_key",
    model="gpt-4o",
    mcp_config_path="./mcp_config.json",
    tool_choice="auto"
)

# With reasoning augmentation
reasoner_config = LlmConfig(
    provider="groq",
    api_key="your_groq_key",
    model="deepseek-r1-distill-llama-70b"
)

config = LlmConfig(
    provider="openai",
    api_key="your_openai_key",
    model="gpt-4o",
    reasoner=reasoner_config
)
```

### Loading from YAML File

```python
import os
from aicore.config import Config
from aicore.llm import Llm

# Set config path via environment variable
os.environ["CONFIG_PATH"] = "./config/config.yml"

# Load and use
config = Config.from_yaml()
llm = Llm.from_config(config.llm)
```

---

## MCP Integration

### Supported MCP Servers (Examples)

| Server | Description | Transport |
|--------|-------------|-----------|
| `@modelcontextprotocol/server-brave-search` | Brave web search | stdio |
| `codetide-mcp-server` | Code analysis | stdio |
| `@upstash/context7-mcp` | Context management | stdio |
| `mcp-remote` (Exa) | AI search | SSE/remote |

### MCP Configuration File

```json
{
  "mcpServers": {
    "brave-search": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-brave-search"],
      "env": {
        "BRAVE_API_KEY": "your-brave-api-key"
      }
    },
    "codetide": {
      "command": "uvx",
      "args": ["--from", "codetide", "codetide-mcp-server"],
      "env": {
        "CODETIDE_WORKSPACE": "./"
      }
    },
    "context7": {
      "command": "npx",
      "args": ["-y", "@upstash/context7-mcp@latest"]
    },
    "exa": {
      "command": "npx",
      "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=your-exa-api-key"]
    },
    "websocket-server": {
      "transport_type": "ws",
      "url": "ws://localhost:8080",
      "description": "Custom WebSocket MCP server"
    },
    "sse-server": {
      "transport_type": "sse",
      "url": "https://api.example.com/mcp/sse",
      "headers": {"Authorization": "Bearer token"}
    }
  }
}
```

### Transport Types

| Transport | Configuration Fields |
|-----------|----------------------|
| **stdio** | `command`, `args`, `env`, `cwd` |
| **WebSocket (ws)** | `url` |
| **SSE** | `url`, `headers` |

### Using MCP with LLM

```python
import asyncio
from aicore.llm import Llm
from aicore.llm.config import LlmConfig

async def main():
    # Configure LLM with MCP
    config = LlmConfig(
        provider="openai",
        api_key="your_openai_key",
        model="gpt-4o",
        mcp_config_path="./mcp_config.json",
        tool_choice="auto"  # Automatically set when mcp_config_path provided
    )
    
    llm = Llm.from_config(config)
    
    # Make request that can use MCP tools
    response = await llm.acomplete(
        "Search for latest news about AI advancements",
        system_prompt="Use available tools to gather comprehensive information"
    )
    print(response)

asyncio.run(main())
```

### Direct MCP Client Usage

```python
import asyncio
from aicore.llm.mcp.client import MCPClient

async def main():
    # Create MCP client from config
    mcp = MCPClient.from_config("./mcp_config.json")
    
    # Connect to all servers
    await mcp.connect()
    
    # Get all available tools
    all_tools = await mcp.servers.tools
    print(f"Available tools: {[t.name for t in all_tools]}")
    
    # Call a tool
    result = await mcp.servers.call_tool(
        "brave_web_search",
        {"query": "Python programming"}
    )
    print(result)
    
    # Or use with context manager
    async with MCPClient.from_config("./mcp_config.json") as mcp:
        tools = await mcp.servers.tools
        result = await mcp.servers.call_tool("tool_name", {"arg": "value"})

asyncio.run(main())
```

---

## Tool Callback

### ToolExecutionCallback Type

```python
from typing import Dict, Any, Callable

ToolExecutionCallback = Callable[[Dict[str, Any]], None]
```

### Callback Event Structure

The callback receives events at two stages:

**Started Event:**
```python
{
    "stage": "started",
    "tool_name": "brave_web_search",
    "server_name": "brave-search",
    "arguments": {"query": "Python programming"}
}
```

**Concluded Event:**
```python
{
    "stage": "concluded",
    "tool_name": "brave_web_search",
    "server_name": "brave-search",
    "duration": 1.23,  # seconds
    "output": [{"type": "text", "text": "..."}]
}
```

### Passing Tool Callback

```python
import asyncio
from aicore.llm import Llm
from aicore.llm.config import LlmConfig
from aicore.llm.mcp.client import ToolExecutionCallback

def my_tool_callback(event: dict):
    if event["stage"] == "started":
        print(f"Tool {event['tool_name']} started on {event['server_name']}")
    elif event["stage"] == "concluded":
        print(f"Tool {event['tool_name']} completed in {event['duration']:.2f}s")

async def main():
    config = LlmConfig(
        provider="openai",
        api_key="your_key",
        model="gpt-4o",
        mcp_config_path="./mcp_config.json"
    )
    
    llm = Llm.from_config(config)
    
    # Set the callback
    llm.tool_callback = my_tool_callback
    
    # Or access provider directly
    llm.provider.tool_callback = my_tool_callback
    
    response = await llm.acomplete("Search for AI news")

asyncio.run(main())
```

---

## Supported Providers

### LLM Providers

| Provider | Identifier | Example Models |
|----------|------------|----------------|
| **Anthropic** | `anthropic` | claude-3-5-sonnet-20241022, claude-3-opus-20240229 |
| **OpenAI** | `openai` | gpt-4o, gpt-4.5, gpt-5, o1, o3-mini |
| **Google Gemini** | `gemini` | gemini-2.0-flash, gemini-2.5-pro |
| **Mistral** | `mistral` | mistral-large-latest, codestral-latest |
| **Groq** | `groq` | llama-3.3-70b-versatile, deepseek-r1-distill-llama-70b |
| **NVIDIA** | `nvidia` | meta/llama-3.1-405b-instruct, nvidia/llama-3.1-nemotron-70b |
| **OpenRouter** | `openrouter` | openai/gpt-4o, anthropic/claude-3.5-sonnet |
| **DeepSeek** | `deepseek` | deepseek-chat, deepseek-reasoner |
| **xAI Grok** | `grok` | grok-3, grok-3-fast |
| **Z.AI** | `zai` | glm-4.5-flash |
| **Claude Code (local)** | `claude_code` | claude-sonnet-4-5-20250929, claude-opus-4-6, … |
| **Claude Code (remote)** | `remote_claude_code` | claude-sonnet-4-5-20250929, claude-opus-4-6, … |

### Provider-Specific Configuration

**`claude_code` — local CLI provider (no API key required):**
```python
# Prerequisites: npm install -g @anthropic-ai/claude-code && claude login
config = LlmConfig(
    provider="claude_code",
    model="claude-sonnet-4-5-20250929",
    permission_mode="acceptEdits",   # "default" | "acceptEdits" | "bypassPermissions"
    cwd="./project",                 # working directory for the session
    max_turns=10,                    # agentic turn limit (default: unlimited)
    allowed_tools=["Read", "Write", "Edit"],  # restrict available tools
    cli_path="/usr/local/bin/claude",         # override claude binary path
    mcp_config_path="./mcp_config.json"       # pass MCP servers to the agent
)
```

```yaml
# YAML equivalent
llm:
  provider: "claude_code"
  model: "claude-sonnet-4-5-20250929"
  permission_mode: "acceptEdits"
  cwd: "./project"
  max_turns: 10
```

**`remote_claude_code` — connects to a remote `aicore-proxy-server` over HTTP SSE:**
```python
config = LlmConfig(
    provider="remote_claude_code",
    model="claude-sonnet-4-5-20250929",
    base_url="http://your-proxy-host:8080",   # proxy server address
    api_key="your_proxy_bearer_token",        # Bearer token set on the server
    permission_mode="acceptEdits",
    cwd="./project",
    max_turns=10,
    allowed_tools=["Read", "Write", "Edit"],
    skip_health_check=False  # set True to skip GET /health on init
)
```

```yaml
# YAML equivalent
llm:
  provider: "remote_claude_code"
  model: "claude-sonnet-4-5-20250929"
  base_url: "http://your-proxy-host:8080"
  api_key: "your_proxy_bearer_token"
  permission_mode: "acceptEdits"
```

---

## Claude Code Proxy Server

`aicore-proxy-server` is a FastAPI + SSE service that wraps `claude-agent-sdk` and exposes it over HTTP, enabling `remote_claude_code` clients to use a Claude CLI that lives on a different machine.

### Installation

```bash
pip install core-for-ai[claude-server]
```

### Starting the server

```bash
# Local only (no tunnel)
aicore-proxy-server --port 8080 --tunnel none

# With ngrok tunnel (token stored in OS credential store on first use)
aicore-proxy-server --port 8080 --tunnel ngrok

# With Cloudflare tunnel
aicore-proxy-server --port 8080 --tunnel cloudflare

# With SSH reverse tunnel
aicore-proxy-server --port 8080 --tunnel ssh --ssh-host user@remote-host --ssh-remote-port 9090
```

### CLI options

| Option | Default | Description |
|--------|---------|-------------|
| `--port` | `8080` | Port to listen on |
| `--tunnel` | `none` | Tunnel mode: `none`, `ngrok`, `cloudflare`, `ssh` |
| `--token` | auto-generated | Bearer token for authentication |
| `--allowed-cwd` | (none) | Comma-separated list of whitelisted working directories |
| `--ssh-host` | — | `user@host` for SSH tunnel mode |
| `--ssh-remote-port` | — | Remote port for SSH tunnel |

### API endpoints

| Method | Path | Description |
|--------|------|-------------|
| `GET` | `/health` | Health check — returns `{"status": "ok"}` |
| `POST` | `/query` | Execute a prompt; returns an SSE stream of SDK messages |

### ngrok auth token persistence

On first use of `--tunnel ngrok`, the server prompts for the ngrok auth token if `NGROK_AUTH_TOKEN` is not set. The token is stored in the OS credential store:

- **Windows** — Windows Credential Manager (PasswordVault via PowerShell)
- **macOS** — Keychain (`security` CLI)
- **Linux** — Secret Service (`secret-tool`)
- **Fallback** — `keyring` Python library

On subsequent runs the token is retrieved automatically.

### Embedding Providers

| Provider | Identifier | Example Models |
|----------|------------|----------------|
| **OpenAI** | `openai` | text-embedding-3-small, text-embedding-3-large |
| **Mistral** | `mistral` | mistral-embed |
| **Groq** | `groq` | nomic-embed-text |
| **Gemini** | `gemini` | text-embedding-004 |
| **NVIDIA** | `nvidia` | nvidia/nv-embed-v1 |

---

## Costs & Usage Tracking

### UsageInfo Class

Access usage through `llm.usage`:

```python
from aicore.llm import Llm
from aicore.llm.config import LlmConfig

config = LlmConfig(provider="openai", api_key="key", model="gpt-4o")
llm = Llm.from_config(config)

response = llm.complete("Hello world")

# Access usage information
usage = llm.usage
print(usage)  # Total | Cost: $0.0023 | Tokens: 342

# Detailed access
print(f"Total tokens: {usage.total_tokens}")
print(f"Total cost: ${usage.total_cost:.4f}")
print(f"Completions count: {len(usage.completions)}")

# Latest completion details
latest = usage.latest_completion
print(f"Prompt tokens: {latest.prompt_tokens}")
print(f"Response tokens: {latest.response_tokens}")
print(f"Cached tokens: {latest.cached_tokens}")
print(f"Cost: ${latest.cost:.6f}")
```

### CompletionUsage Fields

| Field | Type | Description |
|-------|------|-------------|
| `completion_id` | str | Unique identifier for the completion |
| `prompt_tokens` | int | Input tokens used |
| `response_tokens` | int | Output tokens generated |
| `cached_tokens` | int | Tokens served from cache |
| `cache_write_tokens` | int | Tokens written to cache |
| `cost` | float | Calculated cost in USD |
| `total_tokens` | int | Computed: prompt + response tokens |

### Automatic Pricing

Pricing is automatically loaded from `aicore/models_metadata.json`:

```python
# Pricing is auto-loaded based on provider-model combination
config = LlmConfig(provider="openai", model="gpt-4o")
llm = Llm.from_config(config)

# Access pricing config
pricing = llm.usage.pricing
print(f"Input: ${pricing.input}/1M tokens")
print(f"Output: ${pricing.output}/1M tokens")
```

### Custom Pricing

```python
# Set custom pricing rates (per 1M tokens)
llm.usage.set_pricing(input_1m=10.0, output_1m=30.0)

# Or via PricingConfig
from aicore.models_metadata import PricingConfig

config = LlmConfig(
    provider="openai",
    model="gpt-4o",
    pricing=PricingConfig(
        input=5.0,   # $5 per 1M input tokens
        output=15.0  # $15 per 1M output tokens
    )
)
```

---

## Observability

### LlmOperationCollector

The collector automatically tracks all LLM operations:

```python
from aicore.observability.collector import LlmOperationCollector

# Create collector with default JSON storage
collector = LlmOperationCollector.fom_observable_storage_path(
    storage_path="./observability_data"
)

# Or use environment variable
# OBSERVABILITY_DATA_ROOT=./observability_data
collector = LlmOperationCollector.fom_observable_storage_path()
```

### LlmOperationRecord Fields

| Field | Type | Description |
|-------|------|-------------|
| `session_id` | str | Session identifier |
| `workspace` | str | Workspace name |
| `agent_id` | str | Agent identifier |
| `action_id` | str | Action identifier |
| `operation_id` | str | Unique operation ID (ULID) |
| `timestamp` | str | ISO timestamp |
| `operation_type` | str | "completion", "acompletion", "acompletion.tool_call" |
| `provider` | str | Provider name |
| `model` | str | Model name (computed) |
| `system_prompt` | str | System prompt used (computed) |
| `user_prompt` | str | Last user message (computed) |
| `response` | str | Model response |
| `input_tokens` | int | Prompt tokens |
| `output_tokens` | int | Response tokens |
| `cached_tokens` | int | Cached tokens |
| `total_tokens` | int | Total tokens (computed) |
| `cost` | float | Operation cost |
| `latency_ms` | float | Response latency |
| `error_message` | str | Error if failed |
| `success` | bool | Operation success (computed) |

### JSON Storage Configuration

Default storage uses chunked JSON files organized by session:

```
observability_data/
├── session_id_1/
│   ├── 0.json
│   ├── 1.json
│   └── fallback/
├── session_id_2/
│   └── 0.json
└── llm_operations.json
```

**Environment Variables:**

| Variable | Default | Description |
|----------|---------|-------------|
| `OBSERVABILITY_DATA_ROOT` | `observability_data` | Root directory for data |
| `OBSERVABILITY_CHUNK_SIZE` | `50` | Records per chunk file |
| `OBSERVABILITY_WRITE_BUFFER_SIZE` | `5` | Write buffer size |
| `OBSERVABILITY_DISABLE_JSON` | `false` | Disable JSON storage |

### SQL Database Configuration

For production, use SQL storage:

```bash
# Install SQL support
pip install core-for-ai[sql]
```

**Environment Variables:**

| Variable | Description |
|----------|-------------|
| `CONNECTION_STRING` | Sync database connection string |
| `ASYNC_CONNECTION_STRING` | Async database connection string |
| `DB_POOL_SIZE` | Connection pool size (default: 10) |
| `DB_MAX_OVERFLOW` | Max overflow connections (default: 20) |
| `DB_POOL_TIMEOUT` | Pool timeout seconds (default: 30) |
| `DB_POOL_RECYCLE` | Connection recycle seconds (default: 3600) |
| `DB_POOL_PRE_PING` | Enable pre-ping (default: true) |

**Example connection strings:**

```bash
# PostgreSQL
CONNECTION_STRING=postgresql://user:pass@localhost:5432/aicore
ASYNC_CONNECTION_STRING=postgresql+asyncpg://user:pass@localhost:5432/aicore

# SQL Server
CONNECTION_STRING=mssql+pyodbc://user:pass@server:1433/db?driver=ODBC+Driver+17+for+SQL+Server
ASYNC_CONNECTION_STRING=mssql+aioodbc://user:pass@server:1433/db?driver=ODBC+Driver+17+for+SQL+Server
```

### Polars DataFrame Export

```python
import polars as pl
from aicore.observability.collector import LlmOperationCollector

# Load all records as Polars DataFrame
df = LlmOperationCollector.polars_from_file(
    storage_path="./observability_data"
)

# Filter by session
df = LlmOperationCollector.polars_from_file(
    storage_path="./observability_data",
    session_id="specific_session_id"
)

# Query operations
recent = df.filter(pl.col("timestamp") > "2025-01-01")
errors = df.filter(pl.col("error_message") != "")
expensive = df.filter(pl.col("cost") > 0.01).sort("cost", descending=True)
```

### Observability Dashboard

```python
from aicore.observability import ObservabilityDashboard

# Create and run dashboard
dashboard = ObservabilityDashboard(storage="observability_data")
dashboard.run_server(port=8050)
```

Dashboard features:
- Requests per minute
- Average response time
- Token usage trends
- Error rates
- Cost projections
- Provider/model breakdown

---

## Environment Variables Summary

| Variable | Description |
|----------|-------------|
| `CONFIG_PATH` | Path to YAML config file |
| `MCP_JSON_PATH` | Path to MCP config JSON |
| `WORKSPACE` | Default workspace name |
| `AICORE_TIMEOUT` | Default request timeout (seconds) |
| `OBSERVABILITY_DATA_ROOT` | Observability data directory |
| `OBSERVABILITY_DISABLE_JSON` | Disable JSON storage |
| `CONNECTION_STRING` | SQL database connection string |
| `ASYNC_CONNECTION_STRING` | Async SQL connection string |
| `NGROK_AUTH_TOKEN` | ngrok auth token for `aicore-proxy-server --tunnel ngrok` (stored in OS credential store after first prompt) |

---

## Multi-Turn Conversation History with `as_message_records`

When building agentic flows, the **application** is responsible for maintaining conversation context across turns. AiCore supports this via the `as_message_records=True` parameter on `acomplete()`, which returns the full list of messages from the current execution — including tool calls and tool results — as OpenAI-style message dicts.

You can then append a new user message to that list and pass it back as the `prompt` for the next call, preserving the entire conversation chain.

> Full working example: [examples/async_llm_call_with_mcp.py](https://raw.githubusercontent.com/BrunoV21/AiCore/refs/heads/main/examples/async_llm_call_with_mcp.py)

### Message Record Format

Each record is a dict with a `role` key:

| Role | Fields | Description |
|------|--------|-------------|
| `assistant` | `content`, `tool_calls` (optional) | Model response text and/or tool invocations |
| `tool` | `tool_call_id`, `content` | Result returned by a tool execution |

### Example

```python
import asyncio
from aicore.llm import Llm
from aicore.llm.config import LlmConfig

async def main():
    config = LlmConfig(
        provider="openai",
        api_key="your_key",
        model="gpt-4o",
        mcp_config_path="./mcp_config.json",
    )
    llm = Llm.from_config(config)

    # Turn 1 — get message records back
    conversation = await llm.acomplete(
        prompt="Search for the latest AI news",
        system_prompt="Use available tools to find information.",
        as_message_records=True,
    )
    # conversation is a list like:
    # [
    #   {"role": "assistant", "content": null, "tool_calls": [...]},
    #   {"role": "tool", "tool_call_id": "call_abc", "content": "..."},
    #   {"role": "assistant", "content": "Here are the results..."},
    # ]

    # Turn 2 — append a follow-up and send the whole history back
    conversation.append({
        "role": "user",
        "content": "Which of those stories will have the biggest long-term impact?",
    })

    follow_up = await llm.acomplete(
        prompt=conversation,
        system_prompt="Use available tools to find information.",
        as_message_records=True,
    )

    # Merge into a single timeline
    full_history = conversation + follow_up
    print(f"Total messages in history: {len(full_history)}")

asyncio.run(main())
```

This pattern works with all providers (`openai`, `anthropic`, `claude_code`, `remote_claude_code`, etc.) and is the recommended way to build multi-turn agentic workflows where AiCore handles execution and the application owns the conversation state.

---

## Quick Reference

```python
# Minimal example
from aicore.llm import Llm
from aicore.llm.config import LlmConfig

config = LlmConfig(provider="openai", api_key="key", model="gpt-4o")
llm = Llm.from_config(config)
response = llm.complete("Hello")

# With MCP
config = LlmConfig(
    provider="openai",
    api_key="key",
    model="gpt-4o",
    mcp_config_path="./mcp_config.json"
)
llm = Llm.from_config(config)
response = await llm.acomplete("Search for AI news")

# Claude Code local (no API key)
config = LlmConfig(
    provider="claude_code",
    model="claude-sonnet-4-5-20250929"
)
llm = Llm.from_config(config)
response = await llm.acomplete("Explain this codebase")

# Claude Code remote (via aicore-proxy-server)
config = LlmConfig(
    provider="remote_claude_code",
    model="claude-sonnet-4-5-20250929",
    base_url="http://proxy-host:8080",
    api_key="your_proxy_token"
)
llm = Llm.from_config(config)
response = await llm.acomplete("Explain this codebase")

# Check costs
print(llm.usage.total_cost)

# Access observability
from aicore.observability.collector import LlmOperationCollector
df = LlmOperationCollector.polars_from_file()
```